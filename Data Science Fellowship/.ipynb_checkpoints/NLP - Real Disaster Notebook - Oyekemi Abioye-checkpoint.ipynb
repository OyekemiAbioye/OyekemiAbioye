{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "434dada1",
   "metadata": {},
   "source": [
    "### DATA SCIENCE FELLOWSHIP PRORAM - NATURAL DISASTERS DATASET \n",
    "### Oyekemi Abioye"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da39c856",
   "metadata": {},
   "source": [
    "## Loading and preprocessing the data\n",
    "\n",
    "Before analysing, classifying and transforming data I loaded it. \n",
    "To understand the following about the data: \n",
    "\n",
    "* what format is it in? \n",
    "* wehat recognisable features does it have?\n",
    "\n",
    "I used Python and the spaCy library to load, explore and manipulate the textual data.\n",
    "\n",
    "## Loading the data\n",
    "\n",
    "The dataset (https://www.kaggle.com/competitions/nlp-getting-started/data) contains $7613$ training and $3263$ test data of text from tweets, keywords, and tweets location origins.\n",
    "\n",
    "The `natural_disaster_data` also contains the target column connoting if a tweet is about natural disasters or not i.e `zeros` & `ones`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "521636dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c34445ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c208e480",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0515534c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c7622447",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f6920c",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "The text data loaded, comes as a single sequence of symbols (a string of characters) for each tweet(id).\n",
    "\n",
    "The first step is to conduct brief exploratory analysis to foster an in-depth understanding of the data.\n",
    "\n",
    "### Then\n",
    "1. Removal of URLs, emails & HTML tags\n",
    "2. Removal of special characters\n",
    "3. Removal of numeric values\n",
    "4. Removal of hashtags and handles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c27f50",
   "metadata": {},
   "source": [
    "#### Using `.info()`\n",
    "allows us to understand more about the dataset; how many\n",
    "values there are, and what might be missing.\n",
    "It also tells us about dtypes; these are the pandas data types for each column\n",
    "as interpreted by pandas when reading the CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b645815b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038ba674",
   "metadata": {},
   "source": [
    "#### The `.shape`\n",
    "attribute gives the number of rows and columns it contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e1118927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7613\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape[0])\n",
    "print(train_data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f5e062d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 7613\n",
      "\n",
      "===\n",
      "\n",
      "   id keyword location                                               text  \\\n",
      "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
      "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
      "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
      "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
      "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
      "\n",
      "   target  \n",
      "0       1  \n",
      "1       1  \n",
      "2       1  \n",
      "3       1  \n",
      "4       1  \n",
      "\n",
      "===\n",
      "\n",
      "Total number of keywords: 222\n",
      "\n",
      "===\n",
      "\n",
      "Examples per keywords:\n",
      "\n",
      "fatalities               45\n",
      "deluge                   42\n",
      "armageddon               42\n",
      "sinking                  41\n",
      "damage                   41\n",
      "                         ..\n",
      "forest%20fire            19\n",
      "epicentre                12\n",
      "threat                   11\n",
      "inundation               10\n",
      "radiation%20emergency     9\n",
      "Name: keyword, Length: 221, dtype: int64\n",
      "\n",
      "===\n",
      "\n",
      "Total number of location: 3342\n",
      "\n",
      "===\n",
      "\n",
      "Examples per location:\n",
      "\n",
      "USA                    104\n",
      "New York                71\n",
      "United States           50\n",
      "London                  45\n",
      "Canada                  29\n",
      "                      ... \n",
      "MontrÌ©al, QuÌ©bec       1\n",
      "Montreal                 1\n",
      "ÌÏT: 6.4682,3.18287      1\n",
      "Live4Heed??              1\n",
      "Lincoln                  1\n",
      "Name: location, Length: 3341, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {train_data.shape[0]}')\n",
    "\n",
    "print('\\n===\\n')\n",
    "\n",
    "print(train_data.head())\n",
    "\n",
    "print('\\n===\\n')\n",
    "\n",
    "print(f'Total number of keywords: {len(train_data.keyword.unique())}')\n",
    "\n",
    "print('\\n===\\n')\n",
    "\n",
    "print(f'Examples per keywords:\\n\\n{train_data.keyword.value_counts()}')\n",
    "\n",
    "print('\\n===\\n')\n",
    "\n",
    "print(f'Total number of location: {len(train_data.location.unique())}')\n",
    "\n",
    "print('\\n===\\n')\n",
    "\n",
    "print(f'Examples per location:\\n\\n{train_data.location.value_counts()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36418d6",
   "metadata": {},
   "source": [
    "This signifies the following;\n",
    "Keyword - `fatalities` is the most occuring event with `radiation emergency` as the least.\n",
    "Location - Majority of the tweets originate from `USA`.\n",
    "It also shows a need to apply preprocessing to both the `Keyword` & `location` columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "439e0662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking for duplicates in the data\n",
    "train_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55eff540",
   "metadata": {},
   "source": [
    "checking the distribution of the `target` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d00619b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42b2ac1",
   "metadata": {},
   "source": [
    "**The classes appears well balanced.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d467eb",
   "metadata": {},
   "source": [
    "Removal of URLs, emails & HTML tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "0e1d0b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tags(text):\n",
    "    text = re.sub(r'((?:https?|ftp|file)://[-\\w\\d+=&@#/%?~|!:;\\.,]*)', '', text)\n",
    "    text = re.sub(r'([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\\\.[a-zA-Z0-9-.]+)', '', text)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    return text\n",
    "\n",
    "train_data['clean_text'] = train_data['text'].apply(remove_tags)\n",
    "test_data['clean_text'] = test_data['text'].apply(remove_tags)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e8e470",
   "metadata": {},
   "source": [
    "Removal of special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "79534925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def remove_Specialcharacters(text):\n",
    "    text = re.sub(r'(\\d\\.\\d)|[^\\s\\w]', '', text)\n",
    "    text = ''.join([word for word in text if word in string.printable])\n",
    "    return text\n",
    "\n",
    "train_data['clean_text'] = train_data['clean_text'].apply(remove_Specialcharacters)\n",
    "test_data['clean_text'] = test_data['clean_text'].apply(remove_Specialcharacters)\n",
    "\n",
    "#[ ](?=[ ])|[^-_,A-Za-z0-9 ]+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018fc522",
   "metadata": {},
   "source": [
    "Removal of numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "226beb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(text):\n",
    "    text = re.sub(r'\\b\\d+\\b', '', text)\n",
    "    return text\n",
    "\n",
    "train_data['clean_text'] = train_data['clean_text'].apply(remove_numbers)\n",
    "test_data['clean_text'] = test_data['clean_text'].apply(remove_numbers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261b4b17",
   "metadata": {},
   "source": [
    "Removal of  hashtags and handles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "d42c4f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hastags_handles(text):\n",
    "    text = re.sub(r'@[\\w]+','', text)\n",
    "    text = re.sub(r'#[\\w]+','', text)\n",
    "    return text\n",
    "\n",
    "train_data['clean_text'] = train_data['clean_text'].apply(remove_hastags_handles)\n",
    "test_data['clean_text'] = test_data['clean_text'].apply(remove_hastags_handles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1197277",
   "metadata": {},
   "source": [
    "## Preprocessing Continued\n",
    "\n",
    "### Tokenisation\n",
    "\n",
    "The first preprocessing step is *tokenisation*: splitting the text into words by using whitespace, punctuation marks, or both.\n",
    "\n",
    "## Then\n",
    "\n",
    "### Capitalisation\n",
    "\n",
    "### Lemmatisation\n",
    "A lemmatiser tries to convert different word forms to their base representations (lemmas) by looking up whether the lemma is in the vocabulary of the language in question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "40bcf212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beec56e7",
   "metadata": {},
   "source": [
    "Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "ef025eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenisation(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    return tokens\n",
    "    \n",
    "train_data['clean_text'] = train_data['clean_text'].apply(tokenisation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a1aa7a",
   "metadata": {},
   "source": [
    "Capitalisation - converting string to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "c2801781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capitalisation(text):\n",
    "    text = [x.lower() for x in text]\n",
    "    return text\n",
    "\n",
    "train_data['clean_text'] = train_data['clean_text'].apply(capitalisation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76c9f45",
   "metadata": {},
   "source": [
    "Lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "e7d90acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['our', 'deed', 'be', 'the', 'reason', 'of', 'this', 'earthquake', 'm']\n",
      "['our', 'deeds', 'are', 'the', 'reason', 'of', 'this', 'earthquake', 'm']\n"
     ]
    }
   ],
   "source": [
    "def preprocess(sentence, lemmatise):\n",
    "    text = nlp(sentence)\n",
    "    if lemmatise:\n",
    "        lemmas = [token.lemma_ for token in text]\n",
    "        return lemmas\n",
    "    else:\n",
    "        tokens = [token.text.lower() for token in text]\n",
    "        return tokens\n",
    "                                            \n",
    "print(preprocess(\"Our Deeds are the Reason of this earthquake M\", True))\n",
    "print(preprocess(\"Our Deeds are the Reason of this earthquake M\", False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "1f395171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_lemmas(text):\n",
    "    for token in text:\n",
    "        print(f'{token.text:{12}} {token.pos_:{6}} {token.lemma:<{22}} {token.lemma_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "8340e168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our          PRON   17577015140245084783   our\n",
      "Deeds        NOUN   14439616265815796507   deed\n",
      "are          AUX    10382539506755952630   be\n",
      "the          DET    7425985699627899538    the\n",
      "Reason       NOUN   8276932932706964455    reason\n",
      "of           ADP    886050111519832510     of\n",
      "this         DET    1995909169258310477    this\n",
      "earthquake   NOUN   2311630255468200397    earthquake\n",
      "M            NUM    646772771845179972     m\n"
     ]
    }
   ],
   "source": [
    "t = nlp('Our Deeds are the Reason of this earthquake M')\n",
    "show_lemmas(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ddc730",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
